{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40e9d140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import json\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import os\n",
    "from typing import List, Tuple, Dict\n",
    "import open3d as o3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c3225f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KneeKeypointDataset(Dataset):\n",
    "    \"\"\"Dataset for loading knee point clouds and keypoint annotations.\"\"\"\n",
    "    \n",
    "    def __init__(self, json_file: str, pointcloud_dir: str, max_points: int = 8192, \n",
    "                 surface_sampling_method: str = 'uniform', num_keypoints: int = 5):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            json_file: Path to JSON file with keypoint annotations\n",
    "            pointcloud_dir: Directory containing STL mesh files\n",
    "            max_points: Maximum number of points to sample from each mesh\n",
    "            surface_sampling_method: 'uniform' or 'poisson' for surface sampling\n",
    "            num_keypoints: Number of keypoints to use (useful for data cleaning with fewer keypoints)\n",
    "        \"\"\"\n",
    "        with open(json_file, 'r') as f:\n",
    "            self.annotations = json.load(f)\n",
    "        \n",
    "        self.pointcloud_dir = pointcloud_dir\n",
    "        self.max_points = max_points\n",
    "        self.sampling_method = surface_sampling_method\n",
    "        self.num_keypoints = num_keypoints\n",
    "        \n",
    "        # Validate that annotations have the expected number of keypoints\n",
    "        if len(self.annotations) > 0:\n",
    "            expected_keypoints = len(self.annotations[0]['keypoints'])\n",
    "            if expected_keypoints != num_keypoints:\n",
    "                print(f\"Warning: Expected {num_keypoints} keypoints but found {expected_keypoints} in annotation file\")\n",
    "        \n",
    "        # Default keypoint names (you can modify these based on your specific needs)\n",
    "        self.keypoint_names = ['front', 'left', 'right', 'thigh_center', 'shin_center'][:num_keypoints]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "    \n",
    "    def load_point_cloud(self, model_id: str) -> np.ndarray:\n",
    "        \"\"\"Load mesh from STL file and sample points from surface.\"\"\"\n",
    "        mesh_path = os.path.join(self.pointcloud_dir, f\"{model_id}.stl\")\n",
    "        \n",
    "        # Load mesh from STL file\n",
    "        mesh = o3d.io.read_triangle_mesh(mesh_path)\n",
    "        \n",
    "        # Check if mesh is valid\n",
    "        if len(mesh.vertices) == 0:\n",
    "            raise ValueError(f\"Failed to load mesh from {mesh_path}\")\n",
    "        \n",
    "        # Sample points from mesh surface\n",
    "        # Use more points than needed for better coverage\n",
    "        num_sample_points = self.max_points * 2  # Sample more than we need\n",
    "        \n",
    "        # Method 1: Uniform sampling\n",
    "        #pcd = mesh.sample_points_uniformly(number_of_points=num_sample_points)\n",
    "        \n",
    "        # Alternative method 2: Poisson disk sampling (more even distribution)\n",
    "        pcd = mesh.sample_points_poisson_disk(number_of_points=num_sample_points)\n",
    "        \n",
    "        points = np.asarray(pcd.points)\n",
    "        \n",
    "        # If we didn't get enough points, use fewer\n",
    "        if len(points) < self.max_points:\n",
    "            print(f\"Warning: Only sampled {len(points)} points from mesh {model_id}\")\n",
    "        \n",
    "        return points\n",
    "    \n",
    "    def find_nearest_point_indices(self, points: np.ndarray, keypoint_coords: List[List[float]]) -> List[int]:\n",
    "        \"\"\"Find nearest point indices for each keypoint coordinate.\"\"\"\n",
    "        # Use KNN to find nearest points\n",
    "        nbrs = NearestNeighbors(n_neighbors=1, algorithm='ball_tree').fit(points)\n",
    "        \n",
    "        keypoint_indices = []\n",
    "        for coord in keypoint_coords:\n",
    "            distances, indices = nbrs.kneighbors([coord])\n",
    "            keypoint_indices.append(indices[0][0])\n",
    "        \n",
    "        return keypoint_indices\n",
    "    \n",
    "    def normalize_point_cloud(self, points: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Normalize point cloud to unit sphere.\"\"\"\n",
    "        # Center the point cloud\n",
    "        centroid = np.mean(points, axis=0)\n",
    "        points = points - centroid\n",
    "        \n",
    "        # Scale to unit sphere\n",
    "        max_distance = np.max(np.linalg.norm(points, axis=1))\n",
    "        points = points / max_distance\n",
    "        \n",
    "        return points\n",
    "    \n",
    "    def sample_points(self, points: np.ndarray, keypoint_indices: List[int]) -> Tuple[np.ndarray, List[int]]:\n",
    "        \"\"\"Sample points while preserving keypoint indices.\"\"\"\n",
    "        n_points = len(points)\n",
    "        \n",
    "        if n_points <= self.max_points:\n",
    "            # Pad with zeros if needed\n",
    "            padded_points = np.zeros((self.max_points, 3))\n",
    "            padded_points[:n_points] = points\n",
    "            return padded_points, keypoint_indices\n",
    "        \n",
    "        # Always include keypoint indices in sampling\n",
    "        keypoint_set = set(keypoint_indices)\n",
    "        non_keypoint_indices = [i for i in range(n_points) if i not in keypoint_set]\n",
    "        \n",
    "        # Sample remaining points\n",
    "        n_additional = self.max_points - len(keypoint_indices)\n",
    "        if n_additional > 0:\n",
    "            sampled_indices = np.random.choice(\n",
    "                non_keypoint_indices, \n",
    "                size=min(n_additional, len(non_keypoint_indices)), \n",
    "                replace=False\n",
    "            )\n",
    "            all_indices = list(keypoint_indices) + list(sampled_indices)\n",
    "        else:\n",
    "            all_indices = keypoint_indices\n",
    "        \n",
    "        # Create mapping from old to new indices\n",
    "        old_to_new = {old_idx: new_idx for new_idx, old_idx in enumerate(all_indices)}\n",
    "        new_keypoint_indices = [old_to_new[idx] for idx in keypoint_indices]\n",
    "        \n",
    "        return points[all_indices], new_keypoint_indices\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Dict:\n",
    "        annotation = self.annotations[idx]\n",
    "        model_id = annotation['model_id']\n",
    "        \n",
    "        # Load point cloud\n",
    "        points = self.load_point_cloud(model_id)\n",
    "        \n",
    "        # Extract keypoint coordinates (only use the first num_keypoints)\n",
    "        keypoint_coords = [kp['xyz'] for kp in annotation['keypoints'][:self.num_keypoints]]\n",
    "        \n",
    "        # Find nearest point indices\n",
    "        keypoint_indices = self.find_nearest_point_indices(points, keypoint_coords)\n",
    "        \n",
    "        # Normalize point cloud\n",
    "        points = self.normalize_point_cloud(points)\n",
    "        \n",
    "        # Sample points\n",
    "        points, keypoint_indices = self.sample_points(points, keypoint_indices)\n",
    "        \n",
    "        # Create keypoint labels (one-hot encoded)\n",
    "        keypoint_labels = np.zeros((self.num_keypoints, len(points)))\n",
    "        for i, idx in enumerate(keypoint_indices):\n",
    "            if idx < len(points):  # Safety check\n",
    "                keypoint_labels[i, idx] = 1\n",
    "        \n",
    "        return {\n",
    "            'points': torch.FloatTensor(points),\n",
    "            'keypoint_labels': torch.FloatTensor(keypoint_labels),\n",
    "            'model_id': model_id\n",
    "        }\n",
    "\n",
    "\n",
    "class PointNetFeatureExtractor(nn.Module):\n",
    "    \"\"\"PointNet feature extractor for point cloud processing.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim: int = 3, feature_dim: int = 1024):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Point-wise MLPs\n",
    "        self.conv1 = nn.Conv1d(input_dim, 64, 1)\n",
    "        self.conv2 = nn.Conv1d(64, 128, 1)\n",
    "        self.conv3 = nn.Conv1d(128, feature_dim, 1)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.bn3 = nn.BatchNorm1d(feature_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, 3, num_points)\n",
    "        batch_size, _, num_points = x.size()\n",
    "        \n",
    "        # Point-wise feature extraction\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        \n",
    "        # Global feature\n",
    "        global_feature = torch.max(x, 2, keepdim=True)[0]\n",
    "        \n",
    "        # Concatenate global and local features\n",
    "        global_feature = global_feature.repeat(1, 1, num_points)\n",
    "        x = torch.cat([x, global_feature], dim=1)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class KneeKeypointModel(nn.Module):\n",
    "    \"\"\"Multi-task model for knee keypoint detection.\"\"\"\n",
    "    \n",
    "    def __init__(self, num_keypoints: int = 5, feature_dim: int = 1024):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_keypoints = num_keypoints\n",
    "        self.feature_extractor = PointNetFeatureExtractor(feature_dim=feature_dim)\n",
    "        \n",
    "        # Feature dimension after concatenation\n",
    "        concat_dim = feature_dim * 2\n",
    "        \n",
    "        # Shared layers\n",
    "        self.shared_conv1 = nn.Conv1d(concat_dim, 512, 1)\n",
    "        self.shared_conv2 = nn.Conv1d(512, 256, 1)\n",
    "        self.shared_bn1 = nn.BatchNorm1d(512)\n",
    "        self.shared_bn2 = nn.BatchNorm1d(256)\n",
    "        \n",
    "        # Keypoint-specific heads\n",
    "        self.keypoint_heads = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Conv1d(256, 128, 1),\n",
    "                nn.BatchNorm1d(128),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.5),\n",
    "                nn.Conv1d(128, 1, 1)\n",
    "            ) for _ in range(num_keypoints)\n",
    "        ])\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, num_points, 3)\n",
    "        batch_size, num_points, _ = x.size()\n",
    "        \n",
    "        # Transpose for conv1d\n",
    "        x = x.transpose(2, 1)  # (batch, 3, num_points)\n",
    "        \n",
    "        # Extract features\n",
    "        features = self.feature_extractor(x)\n",
    "        \n",
    "        # Shared processing\n",
    "        x = F.relu(self.shared_bn1(self.shared_conv1(features)))\n",
    "        x = F.relu(self.shared_bn2(self.shared_conv2(x)))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Keypoint predictions\n",
    "        keypoint_outputs = []\n",
    "        for head in self.keypoint_heads:\n",
    "            output = head(x)  # (batch, 1, num_points)\n",
    "            output = output.squeeze(1)  # (batch, num_points)\n",
    "            keypoint_outputs.append(output)\n",
    "        \n",
    "        return torch.stack(keypoint_outputs, dim=1)  # (batch, num_keypoints, num_points)\n",
    "\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"Focal loss for handling class imbalance.\"\"\"\n",
    "    \n",
    "    def __init__(self, alpha: float = 1.0, gamma: float = 2.0):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def forward(self, inputs, targets):\n",
    "        # inputs: (batch, num_keypoints, num_points)\n",
    "        # targets: (batch, num_keypoints, num_points)\n",
    "        \n",
    "        # Apply softmax to get probabilities\n",
    "        #probs = F.softmax(inputs, dim=-1) #Not used anywhere...\n",
    "        \n",
    "        # Compute focal loss\n",
    "        ce_loss = F.cross_entropy(inputs.view(-1, inputs.size(-1)), \n",
    "                                 targets.argmax(dim=-1).view(-1), \n",
    "                                 reduction='none')\n",
    "        \n",
    "        # Get probabilities of true class\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        \n",
    "        # Compute focal loss\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
    "        \n",
    "        return focal_loss.mean()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8a83eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, \n",
    "                train_loader, \n",
    "                val_loader, \n",
    "                num_epochs: int = 100,\n",
    "                learning_rate: float = 0.001, \n",
    "                device: str = 'cuda',\n",
    "                weight_decay: float = 1e-4,\n",
    "                focal_alpha: float = 1.0,\n",
    "                focal_gamma: float = 2.0,\n",
    "                save_path: str = 'best_knee_keypoint_model.pth',\n",
    "                checkpoint_path: str = 'checkpoint.pth',\n",
    "                load_path: str = 'best_knee_keypoint_model.pth',\n",
    "                resume: bool = False):\n",
    "    \n",
    "    model = model.to(device)\n",
    "    criterion = FocalLoss(alpha=focal_alpha, gamma=focal_gamma)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.5)\n",
    "\n",
    "    start_epoch = 0\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    # Resume logic\n",
    "    if resume and os.path.exists(load_path):\n",
    "        print(\"Resuming Training From Checkpoint Saved at: \", load_path)\n",
    "        checkpoint = torch.load(load_path, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        best_val_loss = checkpoint['best_val_loss']\n",
    "        print(\"Loading Checkpoint with best val loss: \", best_val_loss)\n",
    "        print(f\"[INFO] Resumed training from epoch {start_epoch}\")\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            points = batch['points'].to(device)\n",
    "            labels = batch['keypoint_labels'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(points)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f\"Epoch [{epoch}/{num_epochs}] Batch [{batch_idx}] Loss: {loss.item():.4f}\")\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                points = batch['points'].to(device)\n",
    "                labels = batch['keypoint_labels'].to(device)\n",
    "                outputs = model(points)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "        print(f\"Epoch [{epoch}/{num_epochs}] Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        # Save best model separately\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "             \n",
    "             \n",
    "            checkpoint = {\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'best_val_loss': best_val_loss\n",
    "            }\n",
    "            torch.save(checkpoint, checkpoint_path) # save checkpoint\n",
    "            print(f\"[INFO] Checkpoint saved at epoch {epoch} with val loss {best_val_loss:.4f} at location: {checkpoint_path}\")\n",
    "          \n",
    "            torch.save(model.state_dict(), save_path) # save model\n",
    "            print(f\"[INFO] Best model saved at epoch {epoch} with val loss {best_val_loss:.4f} at location: {save_path}\")\n",
    "\n",
    "        # Save checkpoint every 20th epoch\n",
    "        if epoch + 1 % 20 == 0:\n",
    "            print(\"Saving period checkpoint at epoch: \", epoch)\n",
    "            checkpoint = {\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'best_val_loss': best_val_loss\n",
    "            }\n",
    "            torch.save(checkpoint, checkpoint_path[:-4] + f\"-{epoch}.pth\")\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    # Load best weights before returning\n",
    "    model.load_state_dict(torch.load(save_path, map_location=device))\n",
    "    print(f\"[INFO] Training completed. Best model loaded from {save_path}\")\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_model(model, test_loader, device: str = 'cuda'):\n",
    "    \"\"\"Evaluate the model and compute keypoint detection accuracy.\"\"\"\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "    \n",
    "    total_distance_error = 0.0\n",
    "    total_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            points = batch['points'].to(device)\n",
    "            labels = batch['keypoint_labels'].to(device)\n",
    "            \n",
    "            outputs = model(points)\n",
    "            \n",
    "            # Get predicted keypoint indices\n",
    "            pred_indices = torch.argmax(outputs, dim=-1)  # (batch, num_keypoints)\n",
    "            true_indices = torch.argmax(labels, dim=-1)   # (batch, num_keypoints)\n",
    "            \n",
    "            # Compute distance error\n",
    "            batch_size = points.size(0)\n",
    "            for i in range(batch_size):\n",
    "                for j in range(model.num_keypoints):\n",
    "                    pred_point = points[i, pred_indices[i, j]]\n",
    "                    true_point = points[i, true_indices[i, j]]\n",
    "                    distance = torch.norm(pred_point - true_point).item()\n",
    "                    total_distance_error += distance\n",
    "                    total_samples += 1\n",
    "    \n",
    "    avg_distance_error = total_distance_error / total_samples\n",
    "    print(f'Average keypoint distance error: {avg_distance_error:.4f}')\n",
    "    \n",
    "    return avg_distance_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0fc07bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters \n",
    "BATCH_SIZE = 16\n",
    "MAX_POINTS = 8192\n",
    "NUM_KEYPOINTS = 2  \n",
    "LEARNING_RATE = 1e-3\n",
    "NUM_EPOCHS = 200\n",
    "TRAIN_SPLIT = 0.8\n",
    "SURFACE_SAMPLING_METHOD = 'uniform'  # uniform/poisson\n",
    "\n",
    "# Focal loss hyperparam\n",
    "FOCAL_ALPHA = 1.0\n",
    "FOCAL_GAMMA = 2.0\n",
    "WEIGHT_DECAY = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb712a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded with 644 samples\n"
     ]
    }
   ],
   "source": [
    "JSON_FILE = 'knee_annotations/7-2-25/knee_points_4_5_aug.json'\n",
    "STL_DIR = 'scans_3_aug/'\n",
    "MODEL_SAVE_PATH = 'kp-selector-2.pth'\n",
    "\n",
    "# Initialize dataset\n",
    "dataset = KneeKeypointDataset(\n",
    "    json_file=JSON_FILE,\n",
    "    pointcloud_dir=STL_DIR,\n",
    "    max_points=MAX_POINTS,\n",
    "    surface_sampling_method=SURFACE_SAMPLING_METHOD,\n",
    "    num_keypoints=NUM_KEYPOINTS\n",
    ")\n",
    "\n",
    "print(f\"Dataset loaded with {len(dataset)} samples\")\n",
    "#print(f\"Using {NUM_KEYPOINTS} keypoints: {dataset.keypoint_names}\") #Defunct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd1575d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 515, Validation samples: 129\n"
     ]
    }
   ],
   "source": [
    "# Split dataset\n",
    "train_size = int(TRAIN_SPLIT * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}, Validation samples: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86405714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has 1,391,618 parameters\n",
      "Training on device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "model = KneeKeypointModel(num_keypoints=NUM_KEYPOINTS)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model has {total_params:,} parameters\")\n",
    "\n",
    "# Train model\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Training on device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f43f44de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming Training From Checkpoint Saved at:  Checkpoints/checkpoint-2.pth\n",
      "Loading Checkpoint with best val loss:  6.777523093753391\n",
      "[INFO] Resumed training from epoch 34\n",
      "Epoch [34/200] Batch [0] Loss: 4.3674\n",
      "Epoch [34/200] Batch [10] Loss: 4.5175\n",
      "Epoch [34/200] Batch [20] Loss: 5.1051\n",
      "Epoch [34/200] Batch [30] Loss: 5.4694\n",
      "Epoch [34/200] Train Loss: 5.0478 | Val Loss: 6.8383\n",
      "Epoch [35/200] Batch [0] Loss: 5.0563\n",
      "Epoch [35/200] Batch [10] Loss: 5.1594\n",
      "Epoch [35/200] Batch [20] Loss: 5.1396\n",
      "Epoch [35/200] Batch [30] Loss: 4.5255\n",
      "Epoch [35/200] Train Loss: 4.9840 | Val Loss: 6.9233\n",
      "Epoch [36/200] Batch [0] Loss: 4.5896\n",
      "Epoch [36/200] Batch [10] Loss: 4.9773\n",
      "Epoch [36/200] Batch [20] Loss: 5.1654\n",
      "Epoch [36/200] Batch [30] Loss: 5.3400\n",
      "Epoch [36/200] Train Loss: 5.0128 | Val Loss: 6.7814\n",
      "Epoch [37/200] Batch [0] Loss: 4.5986\n",
      "Epoch [37/200] Batch [10] Loss: 4.8843\n",
      "Epoch [37/200] Batch [20] Loss: 5.1386\n",
      "Epoch [37/200] Batch [30] Loss: 4.2230\n",
      "Epoch [37/200] Train Loss: 4.9923 | Val Loss: 7.2163\n",
      "Epoch [38/200] Batch [0] Loss: 5.0005\n",
      "Epoch [38/200] Batch [10] Loss: 4.6858\n",
      "Epoch [38/200] Batch [20] Loss: 4.9166\n",
      "Epoch [38/200] Batch [30] Loss: 4.2953\n",
      "Epoch [38/200] Train Loss: 4.9912 | Val Loss: 6.9411\n",
      "Epoch [39/200] Batch [0] Loss: 4.8148\n",
      "Epoch [39/200] Batch [10] Loss: 5.6457\n",
      "Epoch [39/200] Batch [20] Loss: 4.9164\n",
      "Epoch [39/200] Batch [30] Loss: 5.0772\n",
      "Epoch [39/200] Train Loss: 4.9831 | Val Loss: 6.8053\n",
      "Epoch [40/200] Batch [0] Loss: 4.8184\n",
      "Epoch [40/200] Batch [10] Loss: 5.6805\n",
      "Epoch [40/200] Batch [20] Loss: 4.8450\n",
      "Epoch [40/200] Batch [30] Loss: 4.4948\n",
      "Epoch [40/200] Train Loss: 4.9131 | Val Loss: 6.7900\n",
      "Epoch [41/200] Batch [0] Loss: 4.9980\n",
      "Epoch [41/200] Batch [10] Loss: 5.0009\n",
      "Epoch [41/200] Batch [20] Loss: 4.7001\n",
      "Epoch [41/200] Batch [30] Loss: 5.3070\n",
      "Epoch [41/200] Train Loss: 4.8890 | Val Loss: 6.8942\n",
      "Epoch [42/200] Batch [0] Loss: 4.5204\n",
      "Epoch [42/200] Batch [10] Loss: 5.6579\n",
      "Epoch [42/200] Batch [20] Loss: 4.9929\n",
      "Epoch [42/200] Batch [30] Loss: 5.0383\n",
      "Epoch [42/200] Train Loss: 5.0230 | Val Loss: 7.3705\n",
      "Epoch [43/200] Batch [0] Loss: 4.4075\n",
      "Epoch [43/200] Batch [10] Loss: 4.6586\n",
      "Epoch [43/200] Batch [20] Loss: 4.8450\n",
      "Epoch [43/200] Batch [30] Loss: 4.3099\n",
      "Epoch [43/200] Train Loss: 4.8169 | Val Loss: 7.1061\n",
      "Epoch [44/200] Batch [0] Loss: 4.3405\n",
      "Epoch [44/200] Batch [10] Loss: 4.5783\n",
      "Epoch [44/200] Batch [20] Loss: 5.3412\n",
      "Epoch [44/200] Batch [30] Loss: 4.6776\n",
      "Epoch [44/200] Train Loss: 4.7520 | Val Loss: 7.1791\n",
      "Epoch [45/200] Batch [0] Loss: 4.7369\n",
      "Epoch [45/200] Batch [10] Loss: 4.7483\n",
      "Epoch [45/200] Batch [20] Loss: 4.4841\n",
      "Epoch [45/200] Batch [30] Loss: 4.6803\n",
      "Epoch [45/200] Train Loss: 4.7450 | Val Loss: 7.0693\n",
      "Epoch [46/200] Batch [0] Loss: 4.7504\n",
      "Epoch [46/200] Batch [10] Loss: 4.6076\n",
      "Epoch [46/200] Batch [20] Loss: 4.8641\n",
      "Epoch [46/200] Batch [30] Loss: 4.6849\n",
      "Epoch [46/200] Train Loss: 4.6675 | Val Loss: 7.1596\n",
      "Epoch [47/200] Batch [0] Loss: 4.0437\n",
      "Epoch [47/200] Batch [10] Loss: 4.6208\n",
      "Epoch [47/200] Batch [20] Loss: 5.0495\n",
      "Epoch [47/200] Batch [30] Loss: 4.7418\n",
      "Epoch [47/200] Train Loss: 4.6024 | Val Loss: 7.1746\n",
      "Epoch [48/200] Batch [0] Loss: 4.3736\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trained_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLEARNING_RATE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mWEIGHT_DECAY\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfocal_alpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFOCAL_ALPHA\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfocal_gamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFOCAL_GAMMA\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mkp-selector-2.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCheckpoints/checkpoint-2.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCheckpoints/checkpoint-2.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Evaluate model\u001b[39;00m\n\u001b[1;32m     18\u001b[0m avg_error \u001b[38;5;241m=\u001b[39m evaluate_model(trained_model, val_loader, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "Cell \u001b[0;32mIn[3], line 39\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, num_epochs, learning_rate, device, weight_decay, focal_alpha, focal_gamma, save_path, checkpoint_path, load_path, resume)\u001b[0m\n\u001b[1;32m     36\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     37\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m---> 39\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m     40\u001b[0m     points \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpoints\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     41\u001b[0m     labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeypoint_labels\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/pointnet_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:733\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    730\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    731\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 733\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    734\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    735\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    736\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    737\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    738\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    739\u001b[0m ):\n",
      "File \u001b[0;32m~/pointnet_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:789\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    787\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    788\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    790\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    791\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/pointnet_env/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:50\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[0;32m~/pointnet_env/lib/python3.10/site-packages/torch/utils/data/dataset.py:416\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m~/pointnet_env/lib/python3.10/site-packages/torch/utils/data/dataset.py:416\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "Cell \u001b[0;32mIn[2], line 124\u001b[0m, in \u001b[0;36mKneeKeypointDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    121\u001b[0m model_id \u001b[38;5;241m=\u001b[39m annotation[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Load point cloud\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m points \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_point_cloud\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# Extract keypoint coordinates (only use the first num_keypoints)\u001b[39;00m\n\u001b[1;32m    127\u001b[0m keypoint_coords \u001b[38;5;241m=\u001b[39m [kp[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxyz\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m kp \u001b[38;5;129;01min\u001b[39;00m annotation[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeypoints\u001b[39m\u001b[38;5;124m'\u001b[39m][:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_keypoints]]\n",
      "Cell \u001b[0;32mIn[2], line 39\u001b[0m, in \u001b[0;36mKneeKeypointDataset.load_point_cloud\u001b[0;34m(self, model_id)\u001b[0m\n\u001b[1;32m     36\u001b[0m mesh_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpointcloud_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.stl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Load mesh from STL file\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m mesh \u001b[38;5;241m=\u001b[39m \u001b[43mo3d\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_triangle_mesh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmesh_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Check if mesh is valid\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mesh\u001b[38;5;241m.\u001b[39mvertices) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trained_model = train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    device=device,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    focal_alpha=FOCAL_ALPHA,\n",
    "    focal_gamma=FOCAL_GAMMA,\n",
    "    save_path='kp-selector-2.pth',\n",
    "    checkpoint_path='Checkpoints/checkpoint-2.pth',\n",
    "    load_path = 'Checkpoints/checkpoint-2.pth',\n",
    "    resume=True \n",
    ")\n",
    "\n",
    "# Evaluate model\n",
    "avg_error = evaluate_model(trained_model, val_loader, device=device)\n",
    "print(f\"Final evaluation - Average keypoint distance error: {avg_error:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "19eba496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed! Hyperparameters saved to hyperparameters.json\n"
     ]
    }
   ],
   "source": [
    "# Save hyperparameters for reference\n",
    "hyperparams = {\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'max_points': MAX_POINTS,\n",
    "    'num_keypoints': NUM_KEYPOINTS,\n",
    "    'learning_rate': LEARNING_RATE,\n",
    "    'num_epochs': NUM_EPOCHS,\n",
    "    'focal_alpha': FOCAL_ALPHA,\n",
    "    'focal_gamma': FOCAL_GAMMA,\n",
    "    'weight_decay': WEIGHT_DECAY,\n",
    "    'surface_sampling_method': SURFACE_SAMPLING_METHOD\n",
    "}\n",
    "\n",
    "import json\n",
    "with open('hyperparameters.json', 'w') as f:\n",
    "    json.dump(hyperparams, f, indent=2)\n",
    "\n",
    "print(\"Training completed! Hyperparameters saved to hyperparameters.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2698b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb4fb5ee",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'epoch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Save current checkpoint every epoch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m checkpoint_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcheckpoint.pth\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      4\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m----> 5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mepoch\u001b[49m,\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m: model\u001b[38;5;241m.\u001b[39mstate_dict(),\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptimizer_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m: optimizer\u001b[38;5;241m.\u001b[39mstate_dict(),\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscheduler_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m: scheduler\u001b[38;5;241m.\u001b[39mstate_dict(),\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_val_loss\u001b[39m\u001b[38;5;124m'\u001b[39m: best_val_loss\n\u001b[1;32m     10\u001b[0m }\n\u001b[1;32m     11\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(checkpoint, checkpoint_path)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'epoch' is not defined"
     ]
    }
   ],
   "source": [
    "# Save current checkpoint every epoch\n",
    "checkpoint_path = 'checkpoint.pth'\n",
    "\n",
    "checkpoint = {\n",
    "    'epoch': epoch,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'scheduler_state_dict': scheduler.state_dict(),\n",
    "    'best_val_loss': best_val_loss\n",
    "}\n",
    "torch.save(checkpoint, checkpoint_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1ac60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if resume and os.path.exists(checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    best_val_loss = checkpoint['best_val_loss']\n",
    "    print(f\"Resumed training from epoch {start_epoch}\")\n",
    "else:\n",
    "    start_epoch = 0\n",
    "    best_val_loss = float('inf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b41aa3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a7500c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_keypoints_from_stl(model_path: str, \n",
    "                               stl_file: str,\n",
    "                               num_keypoints: int = 2,\n",
    "                               max_points: int = 8192,\n",
    "                               device: str = 'cuda') -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Predict keypoint coordinates for a single STL file.\n",
    "\n",
    "    Args:\n",
    "        model_path: Path to the trained model weights (.pth).\n",
    "        stl_file: Path to the input .stl mesh.\n",
    "        num_keypoints: Number of keypoints your model predicts.\n",
    "        max_points: Number of points to sample.\n",
    "        device: 'cuda' or 'cpu'.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray of shape (num_keypoints, 3) with predicted XYZ coordinates.\n",
    "    \"\"\"\n",
    "\n",
    "    # === 1. Load trained model ===\n",
    "    model = KneeKeypointModel(num_keypoints=num_keypoints)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # === 2. Load and sample the mesh ===\n",
    "    mesh = o3d.io.read_triangle_mesh(stl_file)\n",
    "    if len(mesh.vertices) == 0:\n",
    "        raise ValueError(f\"Failed to load mesh: {stl_file}\")\n",
    "    \n",
    "    num_sample_points = max_points * 2  # over-sample\n",
    "    pcd = mesh.sample_points_uniformly(number_of_points=num_sample_points)\n",
    "    points = np.asarray(pcd.points)\n",
    "\n",
    "    if len(points) < max_points:\n",
    "        print(f\"Warning: Only {len(points)} points sampled.\")\n",
    "    \n",
    "    # === 3. Normalize to unit sphere ===\n",
    "    centroid = np.mean(points, axis=0)\n",
    "    centered_points = points - centroid\n",
    "    max_distance = np.max(np.linalg.norm(centered_points, axis=1))\n",
    "    normalized_points = centered_points / max_distance\n",
    "\n",
    "    # === 4. If needed, downsample to max_points ===\n",
    "    if len(normalized_points) > max_points:\n",
    "        indices = np.random.choice(len(normalized_points), size=max_points, replace=False)\n",
    "        sampled_points = normalized_points[indices]\n",
    "        orig_points = points[indices]  # save original scale for mapping back\n",
    "    else:\n",
    "        sampled_points = normalized_points\n",
    "        orig_points = points\n",
    "\n",
    "    # === 5. Model inference ===\n",
    "    input_tensor = torch.FloatTensor(sampled_points).unsqueeze(0).to(device)  # (1, N, 3)\n",
    "    preds = model(input_tensor)  # (1, num_keypoints, N)\n",
    "    pred_indices = torch.argmax(preds, dim=-1).squeeze(0).cpu().numpy()  # (num_keypoints,)\n",
    "\n",
    "    # === 6. Get predicted coordinates in world space ===\n",
    "    predicted_xyz = orig_points[pred_indices]  # use un-normalized, un-centered points\n",
    "\n",
    "    return predicted_xyz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a6b12dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted keypoint coordinates (XYZ):\n",
      "0.07283366510445152 0.616037560272247 0.1090340103877114\n",
      "0.13580216405701315 0.3340613392398345 0.06855569193220132\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "predicted_coords = predict_keypoints_from_stl(\n",
    "    model_path='kp-selector-2.pth',\n",
    "    stl_file='scans_3/12325.stl',\n",
    "    num_keypoints=2,\n",
    "    max_points=8192,\n",
    "    device='cuda'\n",
    ")\n",
    "\n",
    "print(\"Predicted keypoint coordinates (XYZ):\")\n",
    "for i in predicted_coords:\n",
    "    print(*i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d980aaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed7295bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import open3d as o3d\n",
    "import json\n",
    "import os\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "def get_ground_truth_keypoints_original_space(scan_id, json_file, stl_dir, max_points=8192, num_keypoints=2, surface_sampling_method='uniform'):\n",
    "    # Load annotations\n",
    "    with open(json_file, 'r') as f:\n",
    "        annotations = json.load(f)\n",
    "    \n",
    "    # Find annotation for this scan_id\n",
    "    annotation = next((a for a in annotations if a['model_id'] == scan_id), None)\n",
    "    if annotation is None:\n",
    "        raise ValueError(f\"No annotation found for scan id {scan_id}\")\n",
    "    \n",
    "    # Load mesh\n",
    "    mesh_path = os.path.join(stl_dir, f\"{scan_id}.stl\")\n",
    "    mesh = o3d.io.read_triangle_mesh(mesh_path)\n",
    "    if len(mesh.vertices) == 0:\n",
    "        raise ValueError(f\"Failed to load mesh from {mesh_path}\")\n",
    "    \n",
    "    # Sample points from mesh surface (original space)\n",
    "    num_sample_points = max_points * 2\n",
    "    if surface_sampling_method == 'uniform':\n",
    "        pcd = mesh.sample_points_uniformly(number_of_points=num_sample_points)\n",
    "    elif surface_sampling_method == 'poisson':\n",
    "        pcd = mesh.sample_points_poisson_disk(number_of_points=num_sample_points)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown sampling method: {surface_sampling_method}\")\n",
    "    points = np.asarray(pcd.points)\n",
    "    \n",
    "    # Get ground truth keypoint coordinates (only use the first num_keypoints)\n",
    "    keypoint_coords = [kp['xyz'] for kp in annotation['keypoints'][:num_keypoints]]\n",
    "    keypoint_coords = np.array(keypoint_coords)\n",
    "    \n",
    "    # Find nearest points in the sampled cloud to each keypoint (original space)\n",
    "    nbrs = NearestNeighbors(n_neighbors=1, algorithm='ball_tree').fit(points)\n",
    "    indices = []\n",
    "    for coord in keypoint_coords:\n",
    "        _, idx = nbrs.kneighbors([coord])\n",
    "        indices.append(idx[0][0])\n",
    "    \n",
    "    # Return the coordinates of the selected points (original mesh space)\n",
    "    selected_points = points[indices]\n",
    "    return selected_points\n",
    "\n",
    "# Example usage:\n",
    "# scan_id = \"your_scan_id_here\"\n",
    "# json_file = \"knee_annotations/7-2-25/knee_points_4_5_flipped.json\"\n",
    "# stl_dir = \"scans_3/\"\n",
    "# selected_points = get_ground_truth_keypoints_original_space(scan_id, json_file, stl_dir)\n",
    "# print(selected_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e01a75b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11453112779819768 0.6198950696475138 0.1063818667684893\n",
      "0.1405521313819846 0.33317970808203484 0.06655535475201872\n"
     ]
    }
   ],
   "source": [
    "#Example usage:\n",
    "scan_id = \"12325\"\n",
    "json_file = \"knee_annotations/7-2-25/knee_points_4_5_aug.json\"\n",
    "stl_dir = \"scans_3_aug/\"\n",
    "selected_points = get_ground_truth_keypoints_original_space(scan_id, json_file, stl_dir)\n",
    "for i in selected_points: print(*i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09044d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pointnet_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
